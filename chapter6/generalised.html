
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>3. Generalised Linear Models &#8212; An introduction to data analysis in Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Hierarchical or mixed models" href="lmm.html" />
    <link rel="prev" title="2. Statistics with statsmodels and scipy.stats" href="statsmodels.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">An introduction to data analysis in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    An introduction to data analysis in Python
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter1/the_basics.html">
   1. The Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter1/data_collections.html">
   2. Data collections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter1/functions.html">
   3. Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter1/loops.html">
   4. Loops
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NumPy
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter2/imports.html">
   1. Starting data handling in Python - NumPy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter2/basic_numpy.html">
   2. The
   <code class="docutils literal notranslate">
    <span class="pre">
     numpy
    </span>
   </code>
   module
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter2/stats_simulation.html">
   3. Data indexing, simulation, and summary statistics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  pandas
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/pandas_import_describe.html">
   1. The
   <code class="docutils literal notranslate">
    <span class="pre">
     pandas
    </span>
   </code>
   module
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/indexing.html">
   2. Accessing data in Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/booleans.html">
   3. Boolean operations with Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/split_apply_combine.html">
   4. The power of
   <code class="docutils literal notranslate">
    <span class="pre">
     groupby
    </span>
   </code>
   and aggregation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/custom_functions_flow.html">
   5. Custom functions
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  pandas - advanced uses
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter4/apply.html">
   1. Advanced data handling with
   <code class="docutils literal notranslate">
    <span class="pre">
     pandas
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter4/go_long.html">
   2. Data surgery - reshaping data with
   <code class="docutils literal notranslate">
    <span class="pre">
     melt
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter4/go_wide.html">
   3. There and back again - reshaping data with
   <code class="docutils literal notranslate">
    <span class="pre">
     .pivot_table()
    </span>
   </code>
   .
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter4/merge_join.html">
   4. Data surgery - joining DataFrames with
   <code class="docutils literal notranslate">
    <span class="pre">
     pd.concat
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     pd.merge
    </span>
   </code>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Visualisation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter5/matplotlib.html">
   1. Introducing
   <code class="docutils literal notranslate">
    <span class="pre">
     matplotlib
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter5/seaborn.html">
   2. Using
   <code class="docutils literal notranslate">
    <span class="pre">
     seaborn
    </span>
   </code>
   for smoother data visualisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter5/advanced_seaborn.html">
   3. Advanced plotting with
   <code class="docutils literal notranslate">
    <span class="pre">
     seaborn
    </span>
   </code>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistical Analysis
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="pingouin.html">
   1. (Frequentist) Statistics in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statsmodels.html">
   2. Statistics with
   <code class="docutils literal notranslate">
    <span class="pre">
     statsmodels
    </span>
   </code>
   and
   <code class="docutils literal notranslate">
    <span class="pre">
     scipy.stats
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Generalised Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lmm.html">
   4. Hierarchical or mixed models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ijalm.html">
   5.
   <strong>
    Bonus section
   </strong>
   -
   <strong>
    I
   </strong>
   ts
   <strong>
    J
   </strong>
   ust
   <strong>
    A
   </strong>
   <strong>
    L
   </strong>
   inear
   <strong>
    M
   </strong>
   odel
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/chapter6/generalised.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter6/generalised.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/chapter6/generalised.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   3.1. Logistic Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probit-regression">
   3.2. Probit regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ordered-probit-regression">
   3.3. Ordered Probit Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poisson-or-negative-binomial-models">
   3.4. Poisson or Negative Binomial models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#poisson-regression">
     3.4.1. Poisson Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#negative-binomial-regression">
     3.4.2. Negative Binomial Regression
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Generalised Linear Models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   3.1. Logistic Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probit-regression">
   3.2. Probit regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ordered-probit-regression">
   3.3. Ordered Probit Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poisson-or-negative-binomial-models">
   3.4. Poisson or Negative Binomial models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#poisson-regression">
     3.4.1. Poisson Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#negative-binomial-regression">
     3.4.2. Negative Binomial Regression
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
</pre></div>
</div>
</div>
</div>
<section id="generalised-linear-models">
<h1><span class="section-number">3. </span>Generalised Linear Models<a class="headerlink" href="#generalised-linear-models" title="Permalink to this headline">#</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> is far more than a linear regression powerhouse. It is also capable of fitting a range of <em>generalised</em> linear models, that allow users to fit models to non-normal data, such as binary, count, hierarchical models, and generalised estimating equations. We will explore some of these using the in-built datasets from <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. These are located under the <strong>non-formula</strong> import of the package, which exposes a far wider range of functions outside of the nice formula interface we have seen so far.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import general statsmodels</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span> <span class="c1"># traditional alias</span>
</pre></div>
</div>
</div>
</div>
<section id="logistic-regression">
<h2><span class="section-number">3.1. </span>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">#</a></h2>
<p>We will first explore the use of logistic regression, which allows for the modelling of binary data - such as whether a trial was correct/incorrect, a participant was male/female, and so on. For this example we will use the <a class="reference external" href="http://www.rand.org/health/projects/hie.html">RAND Health Insurance Experiment data</a> which contains a set of variables related to health insurance claims that have been used to investigate key questions in health insurance. The variables are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mdvis</span></code> - Number of outpatient visits to an MD</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lncoins</span></code> - ln(coinsurance + 1), 0 &lt;= coninsurance &lt;= 10</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">idp</span></code> - 1 if individual deductible plan, 0 otherwise</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lpi</span></code> - ln(max(1, annual participation incentive payment)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fmde</span></code> - 0 if idp = 1; ln(max(1, MDE/(0.01 coinsurance))) otherwise</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">physlm</span></code> - 1 if the person has a physical limitation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disea</span></code> - number of chronic disease</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hlthg</span></code> - 1 if self-rated health is good</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hlthf</span></code> - 1 if self-rated health is fair</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hlthp</span></code> - 1 if self-rated health is poor</p></li>
</ul>
<p>We will attempt a simple logistic regression model, predicting whether someone has an individual deductible plan, <code class="docutils literal notranslate"><span class="pre">idp</span></code> (coded as 1 if yes, and zero if no) from a few predictors - number of chronic diseases <code class="docutils literal notranslate"><span class="pre">disea</span></code>, number of outpatient visits <code class="docutils literal notranslate"><span class="pre">mdvis</span></code>, and coinsurance, the amount that has to be paid if a claim is made, <code class="docutils literal notranslate"><span class="pre">lncoins</span></code>.</p>
<p>To fit logistic regression models, we can use the <code class="docutils literal notranslate"><span class="pre">logit</span></code> function from the formula <code class="docutils literal notranslate"><span class="pre">smf</span></code> interface, and specify our model with a formula string. But first we load the data (with a slightly unusual syntax) from the non-formula interface:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load RAND data</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">randhie</span><span class="o">.</span><span class="n">load_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>

<span class="c1"># Fit the model</span>
<span class="n">logistic_results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">(</span><span class="s1">&#39;idp ~ disea + mdvis + lncoins&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">rand</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Examine the results</span>
<span class="n">logistic_results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.537159
         Iterations 6
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>idp</td>       <th>  No. Observations:  </th>  <td> 20190</td> 
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 20186</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Tue, 05 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.06261</td>
</tr>
<tr>
  <th>Time:</th>                <td>12:33:28</td>     <th>  Log-Likelihood:    </th> <td> -10845.</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -11570.</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> 
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -0.5700</td> <td>    0.034</td> <td>  -16.585</td> <td> 0.000</td> <td>   -0.637</td> <td>   -0.503</td>
</tr>
<tr>
  <th>disea</th>     <td>    0.0122</td> <td>    0.003</td> <td>    4.865</td> <td> 0.000</td> <td>    0.007</td> <td>    0.017</td>
</tr>
<tr>
  <th>mdvis</th>     <td>   -0.0495</td> <td>    0.005</td> <td>  -10.399</td> <td> 0.000</td> <td>   -0.059</td> <td>   -0.040</td>
</tr>
<tr>
  <th>lncoins</th>   <td>   -0.3259</td> <td>    0.009</td> <td>  -34.907</td> <td> 0.000</td> <td>   -0.344</td> <td>   -0.308</td>
</tr>
</table></div></div>
</div>
<p>Success! But what does all this mean? Logistic regression isn’t as easy to interpret as ordinary least squares, where a single unit increase in the predictor is associated with a coefficient-value change in the outcome. Logistic regression actually models the <em>probability</em> of a positive/yes/one response in the outcome variable, but does so using the <strong>logistic function</strong>, which maps probability space, which is zero to one, to an infinite continuous space of positive and negative.</p>
<p>The way it does this is by converting probabilities to odds, and then taking the logarithm of the odds - the log-odds. All coefficients in logistic regression represent the change in the log-odds of the outcome with a one-unit increase of the predictor.</p>
<p>Let’s try to clear this up by looking at the model. The <code class="docutils literal notranslate"><span class="pre">mdvis</span></code> predictor shows a significant, negative relationship with the outcome on the log-odds scale. We have no idea what this means outside of that when the number of visits go up, the probability of having a deductible plan goes down. But if we <em>exponentiate</em> the coefficient, we undo the logarithm and get the odds back, which are somewhat more interpretable. <code class="docutils literal notranslate"><span class="pre">numpy</span></code> can help us here!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use numpy to exponentiate the coefficients</span>
<span class="c1"># numpy has an .exp function</span>
<span class="c1"># the coefficients are stored in the .params model attribute</span>
<span class="n">odds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logistic_results</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="n">odds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept    0.565516
disea        1.012295
mdvis        0.951673
lncoins      0.721882
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>This is somewhat more interpretable. As the number of medical visits increases by one, the odds of having an individual deductible plan change by 0.95% (or indeed decrease by 5%).</p>
<p>Unlike ordinary least squares, understanding a logistic regression model involves working closely with the predictions. We saw how to get those from an <code class="docutils literal notranslate"><span class="pre">ols</span></code> object above, but logistic regression is a different beast. While it does have a <code class="docutils literal notranslate"><span class="pre">.fittedvalues</span></code> attribute, it represents the predictions on the log-odds scale. To get at our predictions, we can do two things:</p>
<ul class="simple">
<li><p>Apply the reverse transformation using <code class="docutils literal notranslate"><span class="pre">scipy.special.expit</span></code> on the <code class="docutils literal notranslate"><span class="pre">.fittedvalues</span></code> attribute. This is the inverse-log-odds transform (sometimes known as the inverse logit) and will return the probabilities of a positive response.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">.predict</span></code> method of the fitted model to give the probabilites already back-transformed.</p></li>
</ul>
<p>The latter doesn’t invovle an extra import but we will show both:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import scipy function</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="n">expit_results</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">logistic_results</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">)</span>

<span class="c1"># Use prediction</span>
<span class="n">logistic_predictions</span> <span class="o">=</span> <span class="n">logistic_results</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span> <span class="c1"># a no-argument call predicts the data the model was fitted on</span>

<span class="c1"># Are these the same?</span>
<span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">expit_results</span> <span class="o">==</span> <span class="n">logistic_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>These are equivalent operations. Let’s work with the <code class="docutils literal notranslate"><span class="pre">logistic_predictions</span></code> data, and add it directly to the <code class="docutils literal notranslate"><span class="pre">rand</span></code> data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># add a column</span>
<span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logistic_results</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="n">display</span><span class="p">(</span><span class="n">rand</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mdvis</th>
      <th>lncoins</th>
      <th>idp</th>
      <th>lpi</th>
      <th>fmde</th>
      <th>physlm</th>
      <th>disea</th>
      <th>hlthg</th>
      <th>hlthf</th>
      <th>hlthp</th>
      <th>predictions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.118646</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Our model has given us the predictions of whether someone has an individual deductible plan. If you’re not used to logistic regression it may be a surprise that you get probabilities back and not zeros and ones, which is what you have in the observed data. Its actually entirely up to <em>you</em> how you interpret those probabilities and to map them onto the observed data, but its tradition that we assume the outcome variable is a <span class="math notranslate nohighlight">\(Bernoulli\)</span> distributed variable (stats speak for functioning like a coin toss with heads-tails outcomes, which we want to know the probability of heads). We can actually convert the probabilities into zeros and ones with a simple boolean operation, sometimes known as thresholding:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># add zeros/ones</span>
<span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predicted_idp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">.50</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">rand</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mdvis</th>
      <th>lncoins</th>
      <th>idp</th>
      <th>lpi</th>
      <th>fmde</th>
      <th>physlm</th>
      <th>disea</th>
      <th>hlthg</th>
      <th>hlthf</th>
      <th>hlthp</th>
      <th>predictions</th>
      <th>predicted_idp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.118646</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can now examine how well the model did in terms of its predictions, by computing the terrifyingly named <em>confusion matrix</em>. All this represents is how our model did - for example, when a datapoint <em>did</em> have an individual deductible, how many times did the model say it did? What about the reverse - did it say no when it should have said no?</p>
<p>Evaluating the performance of logistic regression models is an entire sub-field of statistics in and of itself. You will sometime see terms like precision, recall, accuracy, true positive rate, false positive rate, and many, many more floating around. All of these refer to - basically - is whether the model is saying ‘yes’ when it should and ‘no’ when it should, and there are many variations along that theme. Personally I find this the most confusing part of statistical practice and find myself always referring to <a class="reference external" href="https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram">this diagram</a> when working here - I am sure you will too!</p>
<p>Let’s use the <code class="docutils literal notranslate"><span class="pre">pd.crosstab</span></code> function to build the confusion matrix, which takes two columns of 1/0 responses and counts the values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Crosstabs can be normalised or not, depending on the use of the `normalize` keyword</span>
<span class="n">confused1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">rand</span><span class="p">[</span><span class="s1">&#39;idp&#39;</span><span class="p">],</span> <span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predicted_idp&#39;</span><span class="p">],</span> <span class="n">margins</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">confused2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">rand</span><span class="p">[</span><span class="s1">&#39;idp&#39;</span><span class="p">],</span> <span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predicted_idp&#39;</span><span class="p">],</span> <span class="n">margins</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">confused1</span><span class="p">,</span> <span class="n">confused2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>predicted_idp</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>idp</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>14939</td>
      <td>2</td>
      <td>14941</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5249</td>
      <td>0</td>
      <td>5249</td>
    </tr>
    <tr>
      <th>All</th>
      <td>20188</td>
      <td>2</td>
      <td>20190</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>predicted_idp</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>idp</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.739921</td>
      <td>0.000099</td>
      <td>0.74002</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.259980</td>
      <td>0.000000</td>
      <td>0.25998</td>
    </tr>
    <tr>
      <th>All</th>
      <td>0.999901</td>
      <td>0.000099</td>
      <td>1.00000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What this hopefully highlights is that, despite obtaining significant coefficients, the predictions of the model here were pretty poor - in fact, our model barely made a positive prediction! Only 2 cases (both wrong, ironically) were predicted as positive.</p>
<p>Classification is a surprisingly difficult area of statistics - be sure to look beyond model fit and p-values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rand</span><span class="o">.</span><span class="n">idp</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.25998018821198615
</pre></div>
</div>
</div>
</div>
</section>
<section id="probit-regression">
<h2><span class="section-number">3.2. </span>Probit regression<a class="headerlink" href="#probit-regression" title="Permalink to this headline">#</a></h2>
<p>If the above discussion of log-odds, odds, conversions to probability via <code class="docutils literal notranslate"><span class="pre">expit</span></code> and the like was confusing, do not worry. That’s because it is. Fortunately, this is statistics, and there’s always another way to think about things. Another type of model that can deal with binary data is known as <strong>probit</strong> regression. In some ways, this is more easily interpretable, but not everyone agrees. Which approach you use is a matter of taste as both logistic regression and probit regression will offer similar levels of predictive power and conclusions - its just their interpretation that is different.</p>
<p>So how does probit regression work? It assumes that the binary data we’ve observed is a manifestation of an unseen, <em>latent</em> variable, and that variable is normally distributed. The aim of probit regression is thus to predict the latent, normal variable by using something akin to ordinary least squares. Getting a latent variable is all well and good, but we need some probabilities. Those are obtained by using the <strong>cumulative distribution function</strong> (cdf) of a standard (mean zero, standard deviation one) normal distribution. The cdf simply tells us, given an input value, the percentage of the normal distribution that is less than the input value. For example, the very peak of a standard normal distribution is at zero - there is a 50% probability a value from a standard normal is less than zero! Let’s confirm that using <code class="docutils literal notranslate"><span class="pre">scipy.stats.norm</span></code> and its <code class="docutils literal notranslate"><span class="pre">.cdf</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import scipy.stats.norm and make a distribution</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="n">standard_normal</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># What is the CDF of 0? That is, what&#39;s the probability a value from a standard normal is less than zero?</span>
<span class="nb">print</span><span class="p">(</span><span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5
</pre></div>
</div>
</div>
</div>
<p>A visualisation can help explain the CDF of the normal distribution, with a few candidate values. The code for the plot is a little tricky, but feel free to examine:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show different CDFs</span>
<span class="n">cdfs</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.96</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.96</span><span class="p">]</span>

<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s1">&#39;poster&#39;</span><span class="p">):</span>
    
    <span class="c1"># Make figure</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;wspace&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s1">&#39;hspace&#39;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">})</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Zip axes and cdfs and plot them</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">cdf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">cdfs</span><span class="p">):</span>
        
        <span class="c1"># Plot the density function</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">:=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">y</span> <span class="o">:=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        
        <span class="c1"># Get CDF</span>
        <span class="n">cudf</span> <span class="o">=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">cudf</span> <span class="o">&lt;</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">cdf</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.2</span><span class="p">)</span>
        
        <span class="c1"># Tidy up</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">cdf</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">yticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;CDF evaluated at </span><span class="si">{</span><span class="n">cdf</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">% of values smaller than </span><span class="si">{</span><span class="n">cdf</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generalised_21_0.png" src="../_images/generalised_21_0.png" />
</div>
</div>
<p>The shaded areas above represent the probability that a value from the distribution is less than the evaluated value. So for example, only 2.5% of samples are smaller than -1.96 in a standard normal. Its this property that allows probit regression to offer probabilities that allow for the classification of binary observations. Lets see how this is done below, which is traditionally simple at this point - we will refit the logistic regression model above using probit regression:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use probit</span>
<span class="n">probit_results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">probit</span><span class="p">(</span><span class="s1">&#39;idp ~ disea + mdvis + lncoins&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">rand</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Probit summary</span>
<span class="n">probit_results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.538534
         Iterations 5
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Probit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>idp</td>       <th>  No. Observations:  </th>   <td> 20190</td>  
</tr>
<tr>
  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>   <td> 20186</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Tue, 05 Jul 2022</td> <th>  Pseudo R-squ.:     </th>   <td>0.06021</td> 
</tr>
<tr>
  <th>Time:</th>                <td>12:33:29</td>     <th>  Log-Likelihood:    </th>  <td> -10873.</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -11570.</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.895e-302</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -0.3798</td> <td>    0.020</td> <td>  -18.608</td> <td> 0.000</td> <td>   -0.420</td> <td>   -0.340</td>
</tr>
<tr>
  <th>disea</th>     <td>    0.0077</td> <td>    0.001</td> <td>    5.225</td> <td> 0.000</td> <td>    0.005</td> <td>    0.011</td>
</tr>
<tr>
  <th>mdvis</th>     <td>   -0.0253</td> <td>    0.002</td> <td>  -10.188</td> <td> 0.000</td> <td>   -0.030</td> <td>   -0.020</td>
</tr>
<tr>
  <th>lncoins</th>   <td>   -0.1813</td> <td>    0.005</td> <td>  -35.678</td> <td> 0.000</td> <td>   -0.191</td> <td>   -0.171</td>
</tr>
</table></div></div>
</div>
<p>Contrasting that with the <code class="docutils literal notranslate"><span class="pre">logistic_results</span></code> object, things will look very similar in terms of significance and direction of effects. However, how we interpret the coefficents is different. We no longer think of changes in log-odds here, but the change in the value of the unobserved, latent variable that gave rise to our binary observations, in much the same way one interprets ordinary least squares regression. Whether this is easier is a matter of taste; but you can actually scale probit coefficients by about 1.6 to get the log-odds given by a logistic regression - try it with the parameters above for the probit and logistic regressions!</p>
<p>Finally, we can examine the predictions of the probit model using the <code class="docutils literal notranslate"><span class="pre">.fittedvalues</span></code> attribute as before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View first 5 fitted values</span>
<span class="n">display</span><span class="p">(</span><span class="n">probit_results</span><span class="o">.</span><span class="n">fittedvalues</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0   -1.110182
1   -1.160742
2   -1.110182
3   -1.110182
4   -1.110182
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>These are not probabilities - these are the predictions of the latent normal variable. To get the probabilities, we can either use the <code class="docutils literal notranslate"><span class="pre">.predict</span></code> method which will convert it for us, or, we can take these fitted values and put them into a standard normal cumulative density function to get the probability of values being smaller than the given value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use norm.cdf</span>
<span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">probit_results</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.13346028, 0.12287333, 0.13346028, ..., 0.1376718 , 0.1376718 ,
       0.14910515])
</pre></div>
</div>
</div>
</div>
<p>And there they are - those probabilities can now be interpreted as the probability an observed binary outcome is zero or one. Traditionally the cutoff is set at .50 like in logistic regression; but this is something a researcher needs to define in advance. Confusion matrices and so on can be computed in the usual way.</p>
</section>
<section id="ordered-probit-regression">
<h2><span class="section-number">3.3. </span>Ordered Probit Regression<a class="headerlink" href="#ordered-probit-regression" title="Permalink to this headline">#</a></h2>
<p>When you have binary data, logistic or probit regression are great tools to have, albeit with different philosophies. There is a particular class of problem, however, that the <em>latent variable</em> approach of probit regression has great clarity for. In psychology, many studies use Likert (“lick-ert”) scales to collect behavioural responses. Participants may rate how much they agree with a statement or have a preference for a particular stimulus on a scale from 1-7 or -3 to +3, or many other variants.</p>
<p>Traditionally, this data is analysed with ordinary least squares. But a moment’s reflection illustrates this could be misguided. Participants are forced to response on whole, <em>ordered</em> numbers, but the spacing between those numbers may not be consistent - there’s no way to say that a 3 and a 4 on a 7-point scale have the same intrinsic difference as a 6 and a 7. If you’re predicting values its also worth making sure the predictions resemble the data, which ordinary least squares <em>won’t</em> give you. Indeed, there is a lot of work from the last few years illustrating the issues with predicting ordinal data using ordinary least squares.</p>
<p>We will investigate the use of <strong>ordinal regression</strong> here, which is capable of solving the above problem. There are many ways to fit a model to ordinal data, but the one we’ll discuss here is a direct continuation of <strong>probit regression</strong>, sometimes called <em>ordered probit regression</em>. The extension is very natural. In probit regression with binary data, we used the cumulative density function of a normal distribution to get probabilities that a datapoint belongs to either class. In ordered probit regression, in addition to estimating the coefficients, we estimate a series of <strong>cutpoints</strong> that carve up the cumulative density function into sections. The model then predicts the latent variable, and we use the cumulative density function to get a probability. Then, we see which <em>region</em> of the distribution the probability falls into. Lets see an example.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s1">&#39;poster&#39;</span><span class="p">):</span>
    
    <span class="c1"># Make figure</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">count</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">yticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    
    <span class="c1"># Plot pdf</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">:=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">y</span> <span class="o">:=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="c1"># Get a few cutpoints</span>
    <span class="n">cutpoints</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
    
    <span class="c1"># Add scatter + shading</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">cutpoints</span><span class="p">,</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">cutpoints</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cutpoints</span><span class="p">,</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.3</span><span class="p">]):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">c</span><span class="p">)),</span> 
                        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        
    <span class="c1"># Get ordinal data</span>
    <span class="n">ordinal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">cutpoints</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    
    <span class="c1"># Plot ordinal</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ordinal</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">count</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;Greys_r&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generalised_30_0.png" src="../_images/generalised_30_0.png" />
</div>
</div>
<p>In this imaginary dataset, we have a 1,000 data points where responses have been recorded on a scale of 1-4. The figure on the left represents the counts of the observed responses - e.g., 3 is the most common, 4 the least, and so on. The right figure shows the standard normal distribution with three points along the curve, and shaded regions.  The points represent the <strong>cutpoints</strong>, which highlight where a response changes from a 1 to a 2, and so on. The shaded regions represent the areas of the cumulative density function - the greater the shaded area, the more probable a response is. Its clear the lighter shaded region in the 0 - 1.5 area is tied with a greater probability of a response, and indeed ‘3’ is the most common response.</p>
<p>In this example, we <em>know</em> the cutpoints. As such, we can work out the probability regions via subtraction, and see which probability regions are associated with a response.</p>
<ul class="simple">
<li><p>For the ‘1’ responses, the CDF up to a cutpoint of -0.75 will give the probability.</p></li>
<li><p>For the ‘2’ responses, the CDF up to -0.75 <em>subtracted</em> from the cutpoint at 0.1 will give the probability.</p></li>
<li><p>For the ‘3’ responses, the CDF up to 0.1 <em>subtracted</em> from the cutpoint at 1.5 will give the probability.</p></li>
<li><p>For the ‘4’ responses, 1 - the CDF up to 1.5 will give the final probability.</p></li>
</ul>
<p>Let’s compute these using the <code class="docutils literal notranslate"><span class="pre">standard_normal</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Known cuts</span>
<span class="n">known_cutpoints</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="n">cut1</span> <span class="o">=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">known_cutpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">cut2</span> <span class="o">=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">known_cutpoints</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">cutpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">cut3</span> <span class="o">=</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">known_cutpoints</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">cutpoints</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">cut4</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">standard_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">known_cutpoints</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">pr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">cut1</span><span class="p">,</span> <span class="n">cut2</span><span class="p">,</span> <span class="n">cut3</span><span class="p">,</span> <span class="n">cut4</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.23 0.31 0.39 0.07]
</pre></div>
</div>
</div>
</div>
<p>The probabilities map cleanly onto the actual observed data.</p>
<p>OK - that’s the latent variable model of ordinal, probit regression. How can we estimate this in Python, and how can we interpret it? We will fit an ordered probit model below, using the <a class="reference external" href="https://www.statsmodels.org/stable/datasets/generated/fair.html">Affairs dataset</a>, which contains data on marital satisfaction (rated on a 1-5 scale), with other variables such as age, years married, and number of children. There are a few more, but we will focus on the latter two.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">affairs</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fair</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>
<span class="n">affairs</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rate_marriage</th>
      <th>age</th>
      <th>yrs_married</th>
      <th>children</th>
      <th>religious</th>
      <th>educ</th>
      <th>occupation</th>
      <th>occupation_husb</th>
      <th>affairs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.0</td>
      <td>32.0</td>
      <td>9.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>17.0</td>
      <td>2.0</td>
      <td>5.0</td>
      <td>0.111111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.0</td>
      <td>27.0</td>
      <td>13.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>14.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>3.230769</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.0</td>
      <td>22.0</td>
      <td>2.5</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>16.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>1.400000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.0</td>
      <td>37.0</td>
      <td>16.5</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>16.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>0.727273</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>27.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>14.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>4.666666</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We will try to predict <code class="docutils literal notranslate"><span class="pre">rate_marriage</span></code> from <code class="docutils literal notranslate"><span class="pre">yrs_married</span></code> and number of <code class="docutils literal notranslate"><span class="pre">children</span></code>. But first, we need to set the <code class="docutils literal notranslate"><span class="pre">rate_marriage</span></code> variable as an ordered category, which we do with pandas <code class="docutils literal notranslate"><span class="pre">pd.Categorical</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert to categorical</span>
<span class="n">affairs</span><span class="p">[</span><span class="s1">&#39;marriage_category&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">affairs</span><span class="p">[</span><span class="s1">&#39;rate_marriage&#39;</span><span class="p">],</span> <span class="n">categories</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">ordered</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">affairs</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rate_marriage</th>
      <th>age</th>
      <th>yrs_married</th>
      <th>children</th>
      <th>religious</th>
      <th>educ</th>
      <th>occupation</th>
      <th>occupation_husb</th>
      <th>affairs</th>
      <th>marriage_category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.0</td>
      <td>32.0</td>
      <td>9.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>17.0</td>
      <td>2.0</td>
      <td>5.0</td>
      <td>0.111111</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.0</td>
      <td>27.0</td>
      <td>13.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>14.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>3.230769</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.0</td>
      <td>22.0</td>
      <td>2.5</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>16.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>1.400000</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.0</td>
      <td>37.0</td>
      <td>16.5</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>16.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>0.727273</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>27.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>14.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>4.666666</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>OK, this looks perfect. Fitting the model is somewhat more involved as ordinal regression isn’t under the usual <code class="docutils literal notranslate"><span class="pre">statsmodels.formula.api</span></code>. We import it from the <code class="docutils literal notranslate"><span class="pre">miscmodels.ordinal_model</span></code> namespace, like so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import ordinal model</span>
<span class="kn">from</span> <span class="nn">statsmodels.miscmodels.ordinal_model</span> <span class="kn">import</span> <span class="n">OrderedModel</span>
</pre></div>
</div>
</div>
</div>
<p>We also can’t fit this directly with a formula as we have done above. Instead, we use the <code class="docutils literal notranslate"><span class="pre">from_formula</span></code> attribute and fit it as normal, specifying the <code class="docutils literal notranslate"><span class="pre">distr</span></code> keyword as ‘probit’.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the model</span>
<span class="n">ordered_probit</span> <span class="o">=</span> <span class="n">OrderedModel</span><span class="o">.</span><span class="n">from_formula</span><span class="p">(</span><span class="s1">&#39;marriage_category ~ yrs_married + children&#39;</span><span class="p">,</span> 
                                           <span class="n">data</span><span class="o">=</span><span class="n">affairs</span><span class="p">,</span> <span class="n">distr</span><span class="o">=</span><span class="s1">&#39;probit&#39;</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">ordered_results</span> <span class="o">=</span> <span class="n">ordered_probit</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Show summary</span>
<span class="n">ordered_results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 1.236414
         Iterations: 267
         Function evaluations: 426
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>OrderedModel Results</caption>
<tr>
  <th>Dep. Variable:</th>     <td>marriage_category</td> <th>  Log-Likelihood:    </th> <td> -7871.0</td> 
</tr>
<tr>
  <th>Model:</th>               <td>OrderedModel</td>    <th>  AIC:               </th> <td>1.575e+04</td>
</tr>
<tr>
  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>1.579e+04</td>
</tr>
<tr>
  <th>Date:</th>              <td>Tue, 05 Jul 2022</td>  <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Time:</th>                  <td>12:33:30</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>No. Observations:</th>       <td>  6366</td>       <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Df Residuals:</th>           <td>  6360</td>       <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Df Model:</th>               <td>     6</td>       <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>yrs_married</th> <td>   -0.0103</td> <td>    0.003</td> <td>   -3.470</td> <td> 0.001</td> <td>   -0.016</td> <td>   -0.004</td>
</tr>
<tr>
  <th>children</th>    <td>   -0.0558</td> <td>    0.015</td> <td>   -3.714</td> <td> 0.000</td> <td>   -0.085</td> <td>   -0.026</td>
</tr>
<tr>
  <th>1/2</th>         <td>   -2.3552</td> <td>    0.045</td> <td>  -52.493</td> <td> 0.000</td> <td>   -2.443</td> <td>   -2.267</td>
</tr>
<tr>
  <th>2/3</th>         <td>   -0.3653</td> <td>    0.052</td> <td>   -6.967</td> <td> 0.000</td> <td>   -0.468</td> <td>   -0.263</td>
</tr>
<tr>
  <th>3/4</th>         <td>   -0.3110</td> <td>    0.029</td> <td>  -10.625</td> <td> 0.000</td> <td>   -0.368</td> <td>   -0.254</td>
</tr>
<tr>
  <th>4/5</th>         <td>   -0.0420</td> <td>    0.018</td> <td>   -2.304</td> <td> 0.021</td> <td>   -0.078</td> <td>   -0.006</td>
</tr>
</table></div></div>
</div>
<p>We get two coefficients for the predictors, but notice here we also get interesting extra rows - <code class="docutils literal notranslate"><span class="pre">1/2</span></code>, <code class="docutils literal notranslate"><span class="pre">2/3</span></code>, <code class="docutils literal notranslate"><span class="pre">3/4</span></code>, <code class="docutils literal notranslate"><span class="pre">4/5</span></code>. These represent the cutpoints - where responses <em>switch</em> from one to another. The coefficients are interpreted in much the same way as probit regression - one unit increases in the predictor are associated with a coefficient-value change in the latent variable. Somewhat frustratingly, the cutpoints are given on a different scale to the underlying latent variable. Fortunately, we can convert them using the models <code class="docutils literal notranslate"><span class="pre">.transform_threshold_params</span></code>. Let’s take a look:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transform the cutpoints</span>
<span class="n">n_cuts</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">ordered_probit</span><span class="o">.</span><span class="n">transform_threshold_params</span><span class="p">(</span><span class="n">ordered_results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="n">n_cuts</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([       -inf, -2.35518476, -1.661222  , -0.92852619,  0.03032953,
               inf])
</pre></div>
</div>
</div>
</div>
<p>Those cutpoints show where along the distribution the switchpoints are. How to recover the predictions of an ordinal model? There is no <code class="docutils literal notranslate"><span class="pre">.fittedvalues</span></code> attribute here, but if we examine the <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> output of the model, we will see something interesting:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the model to predict</span>
<span class="n">ordered_results</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.01806795, 0.06247448, 0.17128938, 0.3623057 , 0.3858625 ],
       [0.01997206, 0.06688539, 0.17825335, 0.36463794, 0.37025126],
       [0.00991585, 0.04105062, 0.13233409, 0.33902752, 0.47767191],
       ...,
       [0.00991585, 0.04105062, 0.13233409, 0.33902752, 0.47767191],
       [0.01261749, 0.04869801, 0.14733056, 0.35007508, 0.44127886],
       [0.00991585, 0.04105062, 0.13233409, 0.33902752, 0.47767191]])
</pre></div>
</div>
</div>
</div>
<p>This array represents the probability that each observation (the rows) belongs to each bin along the sliced up distribution (each column). For example, the first column would be the probability the response to marital satisfaction is ‘1’. To recover actual points, we simply take the maximum, which is perhaps clearer in a pandas dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up ordinal predictions</span>
<span class="n">ordinal_predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ordered_results</span><span class="o">.</span><span class="n">predict</span><span class="p">(),</span>
                                   <span class="n">columns</span><span class="o">=</span><span class="n">affairs</span><span class="p">[</span><span class="s1">&#39;marriage_category&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">categories</span><span class="p">)</span>

<span class="c1"># Show the predictions</span>
<span class="n">ordinal_predictions</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0       5
1       5
2       5
3       4
4       5
       ..
6361    5
6362    5
6363    5
6364    5
6365    5
Length: 6366, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>These can then be compared with the observed data using confusion-matrix based approaches.</p>
</section>
<section id="poisson-or-negative-binomial-models">
<h2><span class="section-number">3.4. </span>Poisson or Negative Binomial models<a class="headerlink" href="#poisson-or-negative-binomial-models" title="Permalink to this headline">#</a></h2>
<p>Though not encountered too often in psychological science, data that are distributed as counts are popular across other fields and are becoming more popular in behavioural research. Count data represent data that are coded as whole numbers (integers) that start at zero and have no upper limit. Normal regression models like ordinary least squares may make negative count predictions, or fractional predictions, which make little sense (what does it mean to have, say, negative counts of a behaviour?).</p>
<p>Fortunately, count data can be modelled relatively simply with either a <span class="math notranslate nohighlight">\(Poisson\)</span> or <span class="math notranslate nohighlight">\(Negative Binomial\)</span> regression. These models are named after the distributions of the same name which describe the probability of count data. The Poisson is a straightforward single-parameter distribution that describes distributions of counts. The single parameter is sometimes called the ‘rate’, and it represents the average of all the count data in a Poisson distribution. So while the Poisson distribution produces whole-integer values, its parameter can be a continuous (positive!) value.</p>
<p>The negative binomial is traditionally used to describe the probability of observing a number of failures before a single success - e.g. how many times must I roll a dice <em>before</em> I get a 3? The number of ‘fails’ (roll != 3) would be modelled by the negative binomial.</p>
<p>Sometimes, the single-parameter Poisson does a good enough job in regression contexts. However sometimes we encounter a phenomenon called <em>over-dispersion</em>, which is where the variability of the counts exceeds the average tendency. Fortunately, the negative binomial can be used instead, as it allows for a second parameter to incorporate the variability.</p>
<p>If this sounds too technical, don’t worry - it really is. The details are not vital; what we need to focus on is if we are dealing with count data, we can start with the Poisson model, check for this overdispersion, and if it appears, switch to the negative binomial.</p>
<section id="poisson-regression">
<h3><span class="section-number">3.4.1. </span>Poisson Regression<a class="headerlink" href="#poisson-regression" title="Permalink to this headline">#</a></h3>
<p>Using the <code class="docutils literal notranslate"><span class="pre">rand</span></code> data, we could think of a different research question, asking whether the number of medical visits <code class="docutils literal notranslate"><span class="pre">mdvis</span></code> (a count variable!) is influenced by the coinsurance payment <code class="docutils literal notranslate"><span class="pre">lncoins</span></code>, as well as the number of chronic diseases, <code class="docutils literal notranslate"><span class="pre">disea</span></code>. Doing this in <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> is very easy - same idea as before; get the formula ready and use the <code class="docutils literal notranslate"><span class="pre">Poisson</span></code> function!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a Poisson regression</span>
<span class="n">poisson_results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="s1">&#39;mdvis ~ lncoins + disea&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">rand</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Summarise</span>
<span class="n">poisson_results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 3.141408
         Iterations 5
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Poisson Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>mdvis</td>      <th>  No. Observations:  </th>  <td> 20190</td> 
</tr>
<tr>
  <th>Model:</th>                <td>Poisson</td>     <th>  Df Residuals:      </th>  <td> 20187</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Tue, 05 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.04835</td>
</tr>
<tr>
  <th>Time:</th>                <td>12:33:30</td>     <th>  Log-Likelihood:    </th> <td> -63425.</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -66647.</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> 
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    0.6474</td> <td>    0.009</td> <td>   75.120</td> <td> 0.000</td> <td>    0.630</td> <td>    0.664</td>
</tr>
<tr>
  <th>lncoins</th>   <td>   -0.0597</td> <td>    0.002</td> <td>  -27.788</td> <td> 0.000</td> <td>   -0.064</td> <td>   -0.055</td>
</tr>
<tr>
  <th>disea</th>     <td>    0.0409</td> <td>    0.001</td> <td>   81.383</td> <td> 0.000</td> <td>    0.040</td> <td>    0.042</td>
</tr>
</table></div></div>
</div>
<p>Great, some results! What do these mean again?</p>
<p>Like logistic regression, Poisson regression uses a special kind of function to transform the outcome variable in order to model it. It uses a log-transform, which means the coefficients are on the log-scale. That is, as the predictor increases, the log-count of the variable increases by one. As before we can exponentiate these:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exponentiate</span>
<span class="n">display</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">poisson_results</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept    1.910517
lncoins      0.942055
disea        1.041777
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>That is somewhat clearer - as the number of diseases increases, so does the count of doctors visits. And, as the premium goes up, the number of visits go down. This makes sense, the pricier it is to get healthcare, the less likely you are to go.</p>
<p>As with other kinds of generalized linear models, its often easier to work with the predictions directly to draw some conclusions. If we call the <code class="docutils literal notranslate"><span class="pre">.predict</span></code> method on fitted Poisson regression model, we will get the predictions back with no transformations needed. If you want to verify its correct you can exponentiate the <code class="docutils literal notranslate"><span class="pre">.fittedvalues</span></code> attribute and you will get the same results!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Call predict and store the predictions in the rand dataframe</span>
<span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predicted_mdvis&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">poisson_results</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="n">display</span><span class="p">(</span><span class="n">rand</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mdvis</th>
      <th>lncoins</th>
      <th>idp</th>
      <th>lpi</th>
      <th>fmde</th>
      <th>physlm</th>
      <th>disea</th>
      <th>hlthg</th>
      <th>hlthf</th>
      <th>hlthp</th>
      <th>predictions</th>
      <th>predicted_idp</th>
      <th>predicted_mdvis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.118646</td>
      <td>0</td>
      <td>2.544425</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Something unusual is going on. The predicted values are floating numbers, <em>not</em> count data. Whats happening here? Much like with ordinary least squares and logistic regression, Poisson regression is predicting the <em>expected</em> value of the observed data point - essentially, what the average value of that data point would be, given its values on the predictors. So what we are really looking at here is the parameter or ‘rate’ of the distribution that the data point came from!</p>
<p>We can expand our fishing adventures by using an alternative function in the results object of a Poisson regression, <code class="docutils literal notranslate"><span class="pre">.predict_prob</span></code>. This function takes the predicted rate (our <code class="docutils literal notranslate"><span class="pre">predicted_mdvis</span></code> variable above, and it does something quite clever:</p>
<ul class="simple">
<li><p>It takes all the whole numbers from zero the maximum count value observed in the data.</p></li>
<li><p>For each datapoint, it takes the predicted rate, and builds a <em>probability mass function</em> from a Poisson distribution with <em>that</em> rate. What’s a probability mass function?! Its just the probability of seeing a specific count value, given a particular Poisson ‘rate’.</p></li>
<li><p>It returns all of those probabilities to us, and we can see what has the highest value - the most probable count value of our data.</p></li>
</ul>
<p>This is going to be a little tricky to unpack, so lets do this step by step in the order above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build a set of values from zero to the maximum observed in our data</span>
<span class="c1"># We add one because of Pythons &#39;up to but don&#39;t include&#39; rule - see chapter 1!</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">rand</span><span class="p">[</span><span class="s1">&#39;mdvis&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Get the probability mass of each datapoint, given the predicted rate!</span>
<span class="n">probability_mass</span> <span class="o">=</span> <span class="n">poisson_results</span><span class="o">.</span><span class="n">predict_prob</span><span class="p">()</span>

<span class="c1"># Put these in a dataframe</span>
<span class="n">prob_mass_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">probability_mass</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">counts</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">prob_mass_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>68</th>
      <th>69</th>
      <th>70</th>
      <th>71</th>
      <th>72</th>
      <th>73</th>
      <th>74</th>
      <th>75</th>
      <th>76</th>
      <th>77</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.078518</td>
      <td>0.199784</td>
      <td>0.254167</td>
      <td>0.21557</td>
      <td>0.137125</td>
      <td>0.069781</td>
      <td>0.029592</td>
      <td>0.010756</td>
      <td>0.003421</td>
      <td>0.000967</td>
      <td>...</td>
      <td>1.203950e-70</td>
      <td>4.439651e-72</td>
      <td>1.613766e-73</td>
      <td>5.783247e-75</td>
      <td>2.043755e-76</td>
      <td>7.123537e-78</td>
      <td>2.449365e-79</td>
      <td>8.309635e-81</td>
      <td>2.782006e-82</td>
      <td>9.192993e-84</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.078518</td>
      <td>0.199784</td>
      <td>0.254167</td>
      <td>0.21557</td>
      <td>0.137125</td>
      <td>0.069781</td>
      <td>0.029592</td>
      <td>0.010756</td>
      <td>0.003421</td>
      <td>0.000967</td>
      <td>...</td>
      <td>1.203950e-70</td>
      <td>4.439651e-72</td>
      <td>1.613766e-73</td>
      <td>5.783247e-75</td>
      <td>2.043755e-76</td>
      <td>7.123537e-78</td>
      <td>2.449365e-79</td>
      <td>8.309635e-81</td>
      <td>2.782006e-82</td>
      <td>9.192993e-84</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 78 columns</p>
</div></div></div>
</div>
<p>OK! Only two datapoints are shown but we have the probability that the datapoint is the count shown in the DataFrame header, given the predicted rate. If we want to collapse this into a single number, all we need to is take index with the maximum value (i.e. , for row one, what is the column with the highest probability?)</p>
<p><code class="docutils literal notranslate"><span class="pre">pandas</span></code> can help here, with the <code class="docutils literal notranslate"><span class="pre">.idxmax</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assign this back to the rand data</span>
<span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predicted_count_mdvis&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob_mass_df</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">rand</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mdvis</th>
      <th>lncoins</th>
      <th>idp</th>
      <th>lpi</th>
      <th>fmde</th>
      <th>physlm</th>
      <th>disea</th>
      <th>hlthg</th>
      <th>hlthf</th>
      <th>hlthp</th>
      <th>predictions</th>
      <th>predicted_idp</th>
      <th>predicted_mdvis</th>
      <th>predicted_count_mdvis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.118646</td>
      <td>0</td>
      <td>2.544425</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>4.61512</td>
      <td>1</td>
      <td>6.907755</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.73189</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.129403</td>
      <td>0</td>
      <td>2.544425</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>OK, now we have predicted counts! What does that look like?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">rand</span><span class="p">[</span><span class="s1">&#39;mdvis&#39;</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">rand</span><span class="p">[</span><span class="s1">&#39;predicted_count_mdvis&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;predicted_count_mdvis&#39;, ylabel=&#39;mdvis&#39;&gt;
</pre></div>
</div>
<img alt="../_images/generalised_62_1.png" src="../_images/generalised_62_1.png" />
</div>
</div>
<p>Not great, sadly. The model is really under-predicting - the observed data goes from 0 to 77, but the predictions only top 20. Is our data over-dispersed? We can check this, but it involves a little manual calculation - nothing too bad, though. First we take the models <em>Pearson residuals</em>, square them, and sum them. Then we divide this by the residual degrees of freedom of the model. If the result is greater than 1, we have evidence of overdispersion and should switch our modelling strategy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute overdispersion</span>
<span class="n">od</span> <span class="o">=</span> <span class="p">(</span><span class="n">poisson_results</span><span class="o">.</span><span class="n">resid_pearson</span> <span class="o">**</span> <span class="mi">2</span> <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">poisson_results</span><span class="o">.</span><span class="n">df_resid</span>

<span class="nb">print</span><span class="p">(</span><span class="n">od</span><span class="p">,</span> <span class="n">od</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.437470502281646 True
</pre></div>
</div>
</div>
</div>
<p>That might explain our not-so-good predictions.</p>
</section>
<section id="negative-binomial-regression">
<h3><span class="section-number">3.4.2. </span>Negative Binomial Regression<a class="headerlink" href="#negative-binomial-regression" title="Permalink to this headline">#</a></h3>
<p>Fitting a negative binomial regression at this point should be intuitive - we simply set up the formula, and pass it directly to the <code class="docutils literal notranslate"><span class="pre">negativebinomial</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the model </span>
<span class="n">negbinom_results</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">negativebinomial</span><span class="p">(</span><span class="s1">&#39;mdvis ~ lncoins + disea&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">rand</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Get summary</span>
<span class="n">negbinom_results</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 2.159078
         Iterations: 13
         Function evaluations: 16
         Gradient evaluations: 16
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>NegativeBinomial Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>mdvis</td>      <th>  No. Observations:  </th>   <td> 20190</td>  
</tr>
<tr>
  <th>Model:</th>           <td>NegativeBinomial</td> <th>  Df Residuals:      </th>   <td> 20187</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Tue, 05 Jul 2022</td> <th>  Pseudo R-squ.:     </th>   <td>0.01374</td> 
</tr>
<tr>
  <th>Time:</th>                <td>12:33:31</td>     <th>  Log-Likelihood:    </th>  <td> -43592.</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -44199.</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.480e-264</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    0.6264</td> <td>    0.020</td> <td>   31.766</td> <td> 0.000</td> <td>    0.588</td> <td>    0.665</td>
</tr>
<tr>
  <th>lncoins</th>   <td>   -0.0633</td> <td>    0.005</td> <td>  -13.482</td> <td> 0.000</td> <td>   -0.072</td> <td>   -0.054</td>
</tr>
<tr>
  <th>disea</th>     <td>    0.0431</td> <td>    0.001</td> <td>   31.612</td> <td> 0.000</td> <td>    0.040</td> <td>    0.046</td>
</tr>
<tr>
  <th>alpha</th>     <td>    1.3365</td> <td>    0.019</td> <td>   70.288</td> <td> 0.000</td> <td>    1.299</td> <td>    1.374</td>
</tr>
</table></div></div>
</div>
<p>The interpretation of the coefficients is much the same as the Poisson regression case, with the estimates representing the change in the log-counts of the outcome variable with a one-unit change in the predictor variable. Exponentiating them casts them on an odds scale.</p>
<p>You may notice the model seems to have an extra estimate - <code class="docutils literal notranslate"><span class="pre">alpha</span></code> - that also has a significance test associated with it. This is the <strong>overdispersion parameter</strong>. Recall that the Poisson distribution has a single parameter, the ‘rate’, or average. If the counts vary a lot, the Poisson will find it difficult to model the variability with a single parameter. The negative binomial distribution has a similar rate or average, but also has an additional parameter, alpha, which controls the variability of the counts. The model tests whether this value is significantly different from zero, which seems to be the case here. It’s another way of examining over-dispersion! Interestingly, if this parameter equals zero, the model is mathematically equivalent to a Poisson regression.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter6"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="statsmodels.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Statistics with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> and <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lmm.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Hierarchical or mixed models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Alex Jones, Ph.D<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>