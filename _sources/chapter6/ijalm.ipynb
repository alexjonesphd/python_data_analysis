{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a740b91b-3135-4419-a4e2-05586144f6ff",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de2246-5f79-4f94-85d6-0f67dab57cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Bonus section** - **I**ts **J**ust **A** **L**inear **M**odel\n",
    "The teaching of statistics, especially in psychology, is presented as a dazzling array of tools - a Swiss army knife of options that is typically overwhelming for new (and old) students. The typical practice involves a rote, algorithmic approach to statistical inference, whereby the confusion is somewhat reduced by selecting the appropriate test from the huge range on offer. The typical conversation goes like this:\n",
    "\n",
    "- Do you want to compare two or more means? Four please!\n",
    "- Do any of observations provide data in the different variables that make up the means? Yes they do!\n",
    "- Actually, do *all* of the observatiins provide data in the different variables? No, only some.\n",
    "- Then you need a mixed-factorial ANOVA. Wait, is that the same as a mixed model? \n",
    "- No. That's a form of regression. \n",
    "- So regression can't be used with groups? \n",
    "- It can.\n",
    "- Oh no.\n",
    "\n",
    "The problem with the algorithmic approach is, while it eases confusion initially (though I would argue that's not really the case) it masks the elegance of statistical inference from learners. In fact, the vast majority of classic statistical inference approaches used across quantitative science are forms of the steadfast ordinary least squares model. Really - everythig is a regression. To use the words of Professor Daniela Witten, its just a linear model (IJALM)!\n",
    "\n",
    "Below I will walk through some examples of common statistical tests in psychology and express them as linear regressions using the `ols` function from `statsmodels`, also showing how the formula interface can be used to modify data in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec182d49-fbaa-47e9-8fcb-b79336a22550",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A t-test is regression\n",
    "The difference between two groups is equivalent to a simple regression, with a single predictor - a predictor which codes which group the observation belongs to. What numbers you use don't matter, as long as they are different. Fortunately, `statsmodels` takes care of that for us if we use a formula - it is smart enough to recode the data! \n",
    "\n",
    "First lets look at a between-groups t-test using `pingouin` on the `tips` dataset, testing for differences in the amount of tips given by male and female patrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c1b9b4-bd0a-49b2-bc45-38f539bdcb62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>dof</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p-val</th>\n",
       "      <th>CI95%</th>\n",
       "      <th>cohen-d</th>\n",
       "      <th>BF10</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T-test</th>\n",
       "      <td>-1.38786</td>\n",
       "      <td>242</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.166456</td>\n",
       "      <td>[-0.62, 0.11]</td>\n",
       "      <td>0.185494</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.282179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              T  dof alternative     p-val          CI95%   cohen-d   BF10  \\\n",
       "T-test -1.38786  242   two-sided  0.166456  [-0.62, 0.11]  0.185494  0.361   \n",
       "\n",
       "           power  \n",
       "T-test  0.282179  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tips\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "# Display result of t-test using pingouin, with no correction on unbalanced groups\n",
    "pg.ttest(tips.loc[tips['sex'] == 'Female', 'tip'], \n",
    "         tips.loc[tips['sex'] == 'Male', 'tip'], correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bdb3ea-e562-46ad-bd88-b276100d065b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 23 Jun 2022</td> <th>  Prob (F-statistic):</th>  <td> 0.166</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:13:58</td>     <th>  Log-Likelihood:    </th> <td> -423.98</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   852.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   242</td>      <th>  BIC:               </th> <td>   859.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>     <td>    3.0896</td> <td>    0.110</td> <td>   28.032</td> <td> 0.000</td> <td>    2.873</td> <td>    3.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex[T.Female]</th> <td>   -0.2562</td> <td>    0.185</td> <td>   -1.388</td> <td> 0.166</td> <td>   -0.620</td> <td>    0.107</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>75.995</td> <th>  Durbin-Watson:     </th> <td>   1.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 194.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.415</td> <th>  Prob(JB):          </th> <td>4.59e-43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.342</td> <th>  Cond. No.          </th> <td>    2.42</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.008\n",
       "Model:                            OLS   Adj. R-squared:                  0.004\n",
       "Method:                 Least Squares   F-statistic:                     1.926\n",
       "Date:                Thu, 23 Jun 2022   Prob (F-statistic):              0.166\n",
       "Time:                        11:13:58   Log-Likelihood:                -423.98\n",
       "No. Observations:                 244   AIC:                             852.0\n",
       "Df Residuals:                     242   BIC:                             859.0\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "Intercept         3.0896      0.110     28.032      0.000       2.873       3.307\n",
       "sex[T.Female]    -0.2562      0.185     -1.388      0.166      -0.620       0.107\n",
       "==============================================================================\n",
       "Omnibus:                       75.995   Durbin-Watson:                   1.950\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              194.975\n",
       "Skew:                           1.415   Prob(JB):                     4.59e-43\n",
       "Kurtosis:                       6.342   Cond. No.                         2.42\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now fit a linear model, predicting tip by sex\n",
    "ttest_linear_mod = smf.ols('tip ~ sex', data=tips).fit()\n",
    "\n",
    "# Show\n",
    "ttest_linear_mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf688f-759d-4189-ab97-9fd9062bc459",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The coefficient of sex, contrasting male to female, has the same *t*-value as the result of a normal *t*-test. Let's take a moment to examine the intercept value - 3.096, and the coefficient value, -0.2562. If we compute the mean tip amount in our dataset by sex, we should find something interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d82990b-c361-4d67-ba9d-466ce6b451e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "Male      3.089618\n",
       "Female    2.833448\n",
       "Name: tip, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sex\n",
       "Male         NaN\n",
       "Female   -0.2562\n",
       "Name: tip, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the mean tip per sex\n",
    "means = tips.groupby('sex')['tip'].mean()\n",
    "display(means)\n",
    "\n",
    "# Now compute the difference between those means, using .diff of pandas, which subtracts row-wise\n",
    "display(means.diff().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97b011-4c33-479e-90ad-087c3600e33e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The coefficient represents the difference between going from the average of the male tip to the average of female tip. The intercept in the model is equal to the male tip mean!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dcfa6-bf03-4a8c-8d44-4f7dd040419b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A paired t-test as regression\n",
    "A paired *t*-test asks whether the *difference* between two conditions, for the same observations providing the data, is statistically significant from zero. We demonstrate this equivlance by first carrying out a paired test, then computing the difference between conditions, and subjecting that variable to a one sample *t*-test. Let's load the `iris` dataset, which has measures on flower dimensions for different species, and look at the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549beec8-e392-4591-a110-5d3e912e714a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>dof</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p-val</th>\n",
       "      <th>CI95%</th>\n",
       "      <th>cohen-d</th>\n",
       "      <th>BF10</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T-test</th>\n",
       "      <td>22.81322</td>\n",
       "      <td>149</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>1.799629e-50</td>\n",
       "      <td>[1.9, 2.27]</td>\n",
       "      <td>1.512468</td>\n",
       "      <td>1.018e+47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               T  dof alternative         p-val        CI95%   cohen-d  \\\n",
       "T-test  22.81322  149   two-sided  1.799629e-50  [1.9, 2.27]  1.512468   \n",
       "\n",
       "             BF10  power  \n",
       "T-test  1.018e+47    1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>dof</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p-val</th>\n",
       "      <th>CI95%</th>\n",
       "      <th>cohen-d</th>\n",
       "      <th>BF10</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T-test</th>\n",
       "      <td>22.81322</td>\n",
       "      <td>149</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>1.799629e-50</td>\n",
       "      <td>[1.9, 2.27]</td>\n",
       "      <td>1.862692</td>\n",
       "      <td>1.018e+47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               T  dof alternative         p-val        CI95%   cohen-d  \\\n",
       "T-test  22.81322  149   two-sided  1.799629e-50  [1.9, 2.27]  1.862692   \n",
       "\n",
       "             BF10  power  \n",
       "T-test  1.018e+47    1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load iris from seaborn\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Compute paired t-test between sepal and petal length\n",
    "normal_t = pg.ttest(iris['sepal_length'], iris['petal_length'], paired=True, correction=False)\n",
    "\n",
    "# Work out the difference and submit to a one sample t-test, using the same function with 'zero' as second input\n",
    "difference = iris['sepal_length'] - iris['petal_length']\n",
    "one_sample = pg.ttest(difference, 0)\n",
    "\n",
    "display(normal_t, one_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c47f1-a126-4270-a149-06ee8d180d82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It should be no surprise that we can analyse this with a regression too. We want to predict the difference between two conditions using the mean of those differences. This will also allow us a demonstration of how, in formulas, you can use functions to alter your variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906369f2-eef8-438f-a301-54a3b72cf280",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the paired-test formula string\n",
    "paired_formula = 'np.subtract(sepal_length, petal_length) ~ 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb19c8-4cb4-4f74-885f-9fa9240062c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This formula allows us to call the `np.subtract` function, which subtracts two vectors, and predict it using the number one - this is regression speak for 'use the mean of the variable'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262df483-4cc3-4508-bd54-eb0ba86a9fe4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.subtract(sepal_length, petal_length)</td> <th>  R-squared:         </th> <td>   0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                              <td>OLS</td>                   <th>  Adj. R-squared:    </th> <td>   0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                        <td>Least Squares</td>              <th>  F-statistic:       </th> <td>     nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                        <td>Thu, 23 Jun 2022</td>             <th>  Prob (F-statistic):</th>  <td>   nan</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                            <td>11:13:58</td>                 <th>  Log-Likelihood:    </th> <td> -229.28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                 <td>   150</td>                  <th>  AIC:               </th> <td>   460.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>                     <td>   149</td>                  <th>  BIC:               </th> <td>   463.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                         <td>     0</td>                  <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>                 <td>nonrobust</td>                <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.0853</td> <td>    0.091</td> <td>   22.813</td> <td> 0.000</td> <td>    1.905</td> <td>    2.266</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>68.329</td> <th>  Durbin-Watson:     </th> <td>   0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  14.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.469</td> <th>  Prob(JB):          </th> <td>0.000682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.795</td> <th>  Cond. No.          </th> <td>    1.00</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                       OLS Regression Results                                      \n",
       "===================================================================================================\n",
       "Dep. Variable:     np.subtract(sepal_length, petal_length)   R-squared:                       0.000\n",
       "Model:                                                 OLS   Adj. R-squared:                  0.000\n",
       "Method:                                      Least Squares   F-statistic:                       nan\n",
       "Date:                                     Thu, 23 Jun 2022   Prob (F-statistic):                nan\n",
       "Time:                                             11:13:58   Log-Likelihood:                -229.28\n",
       "No. Observations:                                      150   AIC:                             460.6\n",
       "Df Residuals:                                          149   BIC:                             463.6\n",
       "Df Model:                                                0                                         \n",
       "Covariance Type:                                 nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      2.0853      0.091     22.813      0.000       1.905       2.266\n",
       "==============================================================================\n",
       "Omnibus:                       68.329   Durbin-Watson:                   0.175\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               14.580\n",
       "Skew:                           0.469   Prob(JB):                     0.000682\n",
       "Kurtosis:                       1.795   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression\n",
    "paired_regression = smf.ols(paired_formula, data=iris).fit()\n",
    "paired_regression.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeba770-fa7a-4f3e-a9c7-99c76bcdd2d3",
   "metadata": {},
   "source": [
    "The same! What is the mean difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80924bd-d35d-4fbd-8ecf-d5f90ad1c234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0853333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c246d3-c440-4aad-bf2b-87e29fde6580",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A one-way ANOVA is regression\n",
    "You may have heard that an ANOVA is a regression. This is true - IJALM - the predictors are simply variables that code which group the data belongs to. Let's examine the `exercise` dataset, which has several between-subjects variables. We will focus on the exercise `kind` variable - there are three groups of participants and each do a different exercise, where their pulse is measured.\n",
    "\n",
    "First, we will analyse it using a simple one way ANOVA, and do follow up tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4beae3d-af54-444e-a532-77c19b90ea50",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>ddof1</th>\n",
       "      <th>ddof2</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>np2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kind</td>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "      <td>31.987038</td>\n",
       "      <td>3.862193e-11</td>\n",
       "      <td>0.423742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source  ddof1  ddof2          F         p-unc       np2\n",
       "0   kind      2     87  31.987038  3.862193e-11  0.423742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pulse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rest</th>\n",
       "      <td>90.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walking</th>\n",
       "      <td>95.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running</th>\n",
       "      <td>113.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pulse\n",
       "kind               \n",
       "rest      90.833333\n",
       "walking   95.200000\n",
       "running  113.066667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load exercise\n",
    "exercise = sns.load_dataset('exercise')\n",
    "\n",
    "# Anova for exercise data, where DV is pulse the the between subjects factor is exercise kind\n",
    "anova = pg.anova(data=exercise, dv='pulse', between='kind')\n",
    "\n",
    "# We will also compute the means to display\n",
    "means = exercise.groupby('kind').agg({'pulse': 'mean'})\n",
    "\n",
    "# Show\n",
    "display(anova, means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53eec3-c8f8-4e58-afc3-c2dde6b9b2cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, lets fit this as a ordinary least squares regression, to illustrate these tests are one and the same. All we need is to specify what we want to predict - pulse - and from what - exercise kind - and the model will take care of everything for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d753f2-0a32-4fbf-bf30-aeb46bbda559",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>pulse</td>      <th>  R-squared:         </th> <td>   0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   31.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 23 Jun 2022</td> <th>  Prob (F-statistic):</th> <td>3.86e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:13:58</td>     <th>  Log-Likelihood:    </th> <td> -345.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    90</td>      <th>  AIC:               </th> <td>   696.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    87</td>      <th>  BIC:               </th> <td>   704.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>   90.8333</td> <td>    2.083</td> <td>   43.610</td> <td> 0.000</td> <td>   86.693</td> <td>   94.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kind[T.walking]</th> <td>    4.3667</td> <td>    2.946</td> <td>    1.482</td> <td> 0.142</td> <td>   -1.488</td> <td>   10.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kind[T.running]</th> <td>   22.2333</td> <td>    2.946</td> <td>    7.548</td> <td> 0.000</td> <td>   16.379</td> <td>   28.088</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.099</td> <th>  Durbin-Watson:     </th> <td>   1.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.011</td> <th>  Jarque-Bera (JB):  </th> <td>   9.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.612</td> <th>  Prob(JB):          </th> <td> 0.00936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.996</td> <th>  Cond. No.          </th> <td>    3.73</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  pulse   R-squared:                       0.424\n",
       "Model:                            OLS   Adj. R-squared:                  0.410\n",
       "Method:                 Least Squares   F-statistic:                     31.99\n",
       "Date:                Thu, 23 Jun 2022   Prob (F-statistic):           3.86e-11\n",
       "Time:                        11:13:58   Log-Likelihood:                -345.27\n",
       "No. Observations:                  90   AIC:                             696.5\n",
       "Df Residuals:                      87   BIC:                             704.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "Intercept          90.8333      2.083     43.610      0.000      86.693      94.973\n",
       "kind[T.walking]     4.3667      2.946      1.482      0.142      -1.488      10.221\n",
       "kind[T.running]    22.2333      2.946      7.548      0.000      16.379      28.088\n",
       "==============================================================================\n",
       "Omnibus:                        9.099   Durbin-Watson:                   1.753\n",
       "Prob(Omnibus):                  0.011   Jarque-Bera (JB):                9.342\n",
       "Skew:                           0.612   Prob(JB):                      0.00936\n",
       "Kurtosis:                       3.996   Cond. No.                         3.73\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "reg_anova = smf.ols('pulse ~ kind', data=exercise).fit()\n",
    "\n",
    "# Display the summary\n",
    "reg_anova.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4e554-6009-416b-804a-b779e923064a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are notable differences between the `pingouin` output and the OLS solution. First, the F statistic and effect size ($R^2$ for regression, partial $\\eta^2$ for ANOVA) are identical. Its the same thing - IJALM explaining variance. But what to make of the coefficients? We have an intercept, and a 'walking' and 'running' coefficient. These coefficients represent the *mean difference* between, say, the walking condition, and the baseline (which here is implicitly *rest*). So if we add the intercept and the walking estimates together, we will get the mean of the walking condition. Try it! Notice that the intercept value is identical to the mean of the rest variable above. All the coefficients represent is moving from 'rest' to the other conditions. The significance tests help you interpret whether those differences are meaningful.\n",
    "\n",
    "We can control what level goes into the intercept, too. We re-fit the model below and specify that 'walking' should be the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c2456a-c768-4af4-bf0c-04fe7f823873",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>pulse</td>      <th>  R-squared:         </th> <td>   0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   31.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 23 Jun 2022</td> <th>  Prob (F-statistic):</th> <td>3.86e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:13:58</td>     <th>  Log-Likelihood:    </th> <td> -345.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    90</td>      <th>  AIC:               </th> <td>   696.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    87</td>      <th>  BIC:               </th> <td>   704.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                           <td></td>                             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                          <td>   95.2000</td> <td>    2.083</td> <td>   45.707</td> <td> 0.000</td> <td>   91.060</td> <td>   99.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(kind, Treatment(reference=\"walking\"))[T.rest]</th>    <td>   -4.3667</td> <td>    2.946</td> <td>   -1.482</td> <td> 0.142</td> <td>  -10.221</td> <td>    1.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(kind, Treatment(reference=\"walking\"))[T.running]</th> <td>   17.8667</td> <td>    2.946</td> <td>    6.066</td> <td> 0.000</td> <td>   12.012</td> <td>   23.721</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.099</td> <th>  Durbin-Watson:     </th> <td>   1.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.011</td> <th>  Jarque-Bera (JB):  </th> <td>   9.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.612</td> <th>  Prob(JB):          </th> <td> 0.00936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.996</td> <th>  Cond. No.          </th> <td>    3.73</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  pulse   R-squared:                       0.424\n",
       "Model:                            OLS   Adj. R-squared:                  0.410\n",
       "Method:                 Least Squares   F-statistic:                     31.99\n",
       "Date:                Thu, 23 Jun 2022   Prob (F-statistic):           3.86e-11\n",
       "Time:                        11:13:58   Log-Likelihood:                -345.27\n",
       "No. Observations:                  90   AIC:                             696.5\n",
       "Df Residuals:                      87   BIC:                             704.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================================================\n",
       "                                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                             95.2000      2.083     45.707      0.000      91.060      99.340\n",
       "C(kind, Treatment(reference=\"walking\"))[T.rest]       -4.3667      2.946     -1.482      0.142     -10.221       1.488\n",
       "C(kind, Treatment(reference=\"walking\"))[T.running]    17.8667      2.946      6.066      0.000      12.012      23.721\n",
       "==============================================================================\n",
       "Omnibus:                        9.099   Durbin-Watson:                   1.753\n",
       "Prob(Omnibus):                  0.011   Jarque-Bera (JB):                9.342\n",
       "Skew:                           0.612   Prob(JB):                      0.00936\n",
       "Kurtosis:                       3.996   Cond. No.                         3.73\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model, but specify the baseline to be 'walking' by specifying a contrast like so\n",
    "reg_anova = smf.ols('pulse ~ C(kind, Treatment(reference=\"walking\"))', data=exercise).fit()\n",
    "\n",
    "# Display the summary\n",
    "reg_anova.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6da843-36cd-4ab0-a29f-bff20a795d46",
   "metadata": {},
   "source": [
    "Notice now that the intercept the mean of walking, and to go to the 'rest' mean we need to subtract 4.3667 units. IJALM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535fc6d-9a53-42b5-b3d9-a872ed6fb899",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Factorial ANOVA's are also regressions\n",
    "You guessed it - even more complex ANOVA designs can also be expressed as regressions, because the linear model approach underpins all of statistics. Interactions between one or more factors are also captured OLS. \n",
    "\n",
    "We can use the `anova_lm` function from statsmodels to show this is the case. This takes a regression model and computes an ANOVA from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe1d393-6197-47df-9a82-59a1243604e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>np2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kind</td>\n",
       "      <td>8326.067</td>\n",
       "      <td>2</td>\n",
       "      <td>4163.033</td>\n",
       "      <td>37.824</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diet</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>1</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>11.465</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kind * diet</td>\n",
       "      <td>815.756</td>\n",
       "      <td>2</td>\n",
       "      <td>407.878</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residual</td>\n",
       "      <td>9245.200</td>\n",
       "      <td>84</td>\n",
       "      <td>110.062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source        SS  DF        MS       F  p-unc    np2\n",
       "0         kind  8326.067   2  4163.033  37.824  0.000  0.474\n",
       "1         diet  1261.878   1  1261.878  11.465  0.001  0.120\n",
       "2  kind * diet   815.756   2   407.878   3.706  0.029  0.081\n",
       "3     Residual  9245.200  84   110.062     NaN    NaN    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anova with Pingouin\n",
    "aov = pg.anova(data=exercise, dv='pulse', between=['kind', 'diet'], \n",
    "               ss_type=1)\n",
    "display(aov.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebcdd7fb-7aab-4267-b1cb-2e586589ebd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8326.067</td>\n",
       "      <td>4163.033</td>\n",
       "      <td>37.824</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>11.465</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind:diet</th>\n",
       "      <td>2.0</td>\n",
       "      <td>815.756</td>\n",
       "      <td>407.878</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>84.0</td>\n",
       "      <td>9245.200</td>\n",
       "      <td>110.062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             df    sum_sq   mean_sq       F  PR(>F)\n",
       "kind        2.0  8326.067  4163.033  37.824   0.000\n",
       "diet        1.0  1261.878  1261.878  11.465   0.001\n",
       "kind:diet   2.0   815.756   407.878   3.706   0.029\n",
       "Residual   84.0  9245.200   110.062     NaN     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the anova_lm function\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Fit a model as a linear regression that includes both kind, diet, and their interaction\n",
    "exercise_regression = smf.ols('pulse ~ kind + diet + kind:diet', exercise).fit()\n",
    "\n",
    "# Conduct the anova\n",
    "reg_anova = anova_lm(exercise_regression)\n",
    "display(reg_anova.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31deb502-f4e6-4c99-902f-98fed40e7b10",
   "metadata": {},
   "source": [
    "What `anova_lm` does is described in more detail below, but hopefully this has convinced you of the fact that statistics IJALM. When you get to working with repeated measures ANOVA or beyond you can switch to hierarchical models to account for the similarity between data points, and the principle is the same. When to use a the linear model or ANOVA approach? Both are the same, but both have their advantages. If you *only* have categorical predictors, an ANOVA is typically more straightforward to use. But if you want to blend categorical and continuous predictors, you can only extend into the linear model approach which will never let you down, once the interpretation is mastered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c05b3-8552-47fa-b4be-7a42a09f8afa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A deep dive into statistics - factorial ANOVA designs as linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816b0d0-01e3-4728-b172-075f7082f3b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With a little (a lot) effort, we can understand the equivalence of things like *t*-test and ANOVA by delving into the foundations of classical statistics, which seeks to explain the *variability* in response variable using other variables. \n",
    "\n",
    "What is variability anyway? Put simply, it is the amount a variable spreads out around its mean. For example, we can plot the `pulse` variable measured in the `exercise` dataset and examine its variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648c1b68-0349-403f-b1bf-9d46702d3c60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show variability of pulse, black dot is the mean\n",
    "figure = sns.stripplot(data=exercise, y='pulse', alpha=0.3)\n",
    "sns.pointplot(data=exercise, y='pulse', color='black', ax=figure)\n",
    "sns.despine(ax=figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94fc82-260a-4b5e-854e-30ae4d771f87",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The variability is clear here. Lots of points are around the mean, with some further away. We can summarise this variability with a single number, which is **vital** in statistics - known as the *total sums of squares*. This number represents how variable a variable is! We compute the total sums of squares by:\n",
    "1. Compute the mean of a variable\n",
    "2. Subtracting the mean from each observation in a variable\n",
    "3. Squaring those values\n",
    "4. Adding them all up\n",
    "\n",
    "As an aside, this sums of squares is the basis of the variance and standard deviation statistics. To get variance, divide the sums of squares by the number of datapoints, minus one. To get the standard deviation, take the square root of the variance.\n",
    "\n",
    "Once we have this variability, we have another vital question - how can we *explain* this variability? Are there other variables that can account for this variability? As you have no doubt guessed, a linear model can do this. Lets see how. First we will compute the total sums of squares of the `pulse` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a746b75a-bfea-43b2-8312-b771bed24904",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19648.9\n"
     ]
    }
   ],
   "source": [
    "# Compute pulse sums of squares\n",
    "total_sums_squares = np.sum((exercise['pulse'] - exercise['pulse'].mean()) ** 2)\n",
    "print(total_sums_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e539b-eebe-43fe-8862-04ca4d0bd921",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Measuring error and $R^2$\n",
    "The implicit way in which almost all statistical tests progress is via a *model comparison*. If we were given the `pulse` variable, and asked to make a guess about what each observation would be, we could use the mean of `pulse`. How would we measure our accuracy, or put another way, how well would we quantify our error using this method? The approach is to take the difference between the actual data and the prediction, known as the **sums of squares residual**, because the 'residuals' are what is 'left over' after our guesses. We take the differences, square them, and add them up.\n",
    "\n",
    "In fact, these two sums of squares - the total and residual - are what we need to compute the $R^2$, using a simple formula - $1 - \\frac{SSR}{SST}$. This tells us the percentage of variation explained in a literal sense. \n",
    "\n",
    "We find an interesting equivalence here. If we use the mean of a variable as our best guess, then our sums of squares residual will be equal to the total sums of squares of a variable! We can fit a model using `statsmodels` and the `ols` class to demonstrate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c22bc1a5-4bf1-43cc-b0e8-ad5316691258",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19648.9 True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Remember that a variable ~ 1 is equivalent to using the mean to predict a variable\n",
    "base_model = smf.ols('pulse ~ 1', exercise).fit()\n",
    "\n",
    "# The model has a 'ssr' attribute that holds our sums of squares residuals\n",
    "print(base_model.ssr, base_model.ssr == total_sums_squares)\n",
    "\n",
    "# And also has a 'centered_tss' attribute that stores its total sums of squares\n",
    "print(base_model.centered_tss == total_sums_squares and base_model.centered_tss == base_model.ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f0197-da37-46f5-8e94-c1f1892a73b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can now compute directly the $R^2$ of our model and compare it to the attribute stored by the model. In this case, the $R^2$ is zero, as we have explained no variability by using the mean as a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c674412-8d03-45af-ba1e-ae78bb20207b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manually compute\n",
    "rsquared = 1 - (base_model.ssr / base_model.centered_tss)\n",
    "\n",
    "# Check against the model\n",
    "print(rsquared == base_model.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973dffd9-d41d-4d95-91eb-679634f92e4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we know that the mean of a variable can be thought of as a 'guess' for the variability of a model, lets try and do better. Someone's pulse is probably determined by the kind of activity they are doing. Luckily, we have this data, so lets add it as a predictor in a new model, and check the summary. This will tell us what the new $R^2$ is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a0edc8-0a80-48d3-9ece-68a01d495066",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>pulse</td>      <th>  R-squared:         </th> <td>   0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   31.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 23 Jun 2022</td> <th>  Prob (F-statistic):</th> <td>3.86e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:13:58</td>     <th>  Log-Likelihood:    </th> <td> -345.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    90</td>      <th>  AIC:               </th> <td>   696.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    87</td>      <th>  BIC:               </th> <td>   704.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>   90.8333</td> <td>    2.083</td> <td>   43.610</td> <td> 0.000</td> <td>   86.693</td> <td>   94.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kind[T.walking]</th> <td>    4.3667</td> <td>    2.946</td> <td>    1.482</td> <td> 0.142</td> <td>   -1.488</td> <td>   10.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kind[T.running]</th> <td>   22.2333</td> <td>    2.946</td> <td>    7.548</td> <td> 0.000</td> <td>   16.379</td> <td>   28.088</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.099</td> <th>  Durbin-Watson:     </th> <td>   1.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.011</td> <th>  Jarque-Bera (JB):  </th> <td>   9.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.612</td> <th>  Prob(JB):          </th> <td> 0.00936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.996</td> <th>  Cond. No.          </th> <td>    3.73</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  pulse   R-squared:                       0.424\n",
       "Model:                            OLS   Adj. R-squared:                  0.410\n",
       "Method:                 Least Squares   F-statistic:                     31.99\n",
       "Date:                Thu, 23 Jun 2022   Prob (F-statistic):           3.86e-11\n",
       "Time:                        11:13:58   Log-Likelihood:                -345.27\n",
       "No. Observations:                  90   AIC:                             696.5\n",
       "Df Residuals:                      87   BIC:                             704.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "Intercept          90.8333      2.083     43.610      0.000      86.693      94.973\n",
       "kind[T.walking]     4.3667      2.946      1.482      0.142      -1.488      10.221\n",
       "kind[T.running]    22.2333      2.946      7.548      0.000      16.379      28.088\n",
       "==============================================================================\n",
       "Omnibus:                        9.099   Durbin-Watson:                   1.753\n",
       "Prob(Omnibus):                  0.011   Jarque-Bera (JB):                9.342\n",
       "Skew:                           0.612   Prob(JB):                      0.00936\n",
       "Kurtosis:                       3.996   Cond. No.                         3.73\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add 'kind' as a predictor\n",
    "with_kind = smf.ols('pulse ~ 1 + kind', exercise).fit()\n",
    "display(with_kind.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3dfdf-c819-4130-83e8-50aa82c32b2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can assume from our large $R^2$ that our sums of squared residuals are lower, which we can confirm and check the $R^2$ manually. Our total sums of squares will be unchanged, as the `pulse` variable is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c9dc46-8a25-473a-9217-e0167a619bd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSR = 11322.833333333332\n",
      "SST = 19648.9\n",
      "R squared = 0.42374212636161157\n"
     ]
    }
   ],
   "source": [
    "# Show SSR and SST\n",
    "print(f'SSR = {with_kind.ssr}\\nSST = {with_kind.centered_tss}')\n",
    "\n",
    "# Directly\n",
    "rsquared = 1 - (with_kind.ssr/with_kind.centered_tss)\n",
    "\n",
    "# Compute Rsquare directly\n",
    "print(f'R squared = {rsquared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aa506-2832-4f74-af1b-e017650d9883",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How much better? A model comparison via differences in error\n",
    "Our sums of squares residual has gone down - we have less error if we use exercise kind as a predictor. Great! But, how much should we care about this? Is this a meaningful change or not? How can we evaluate the usefulness of our predictors? The way to do this is by using a comparison of linear models, which gives us an $F$-ratio - you will have spotted these in the summary, but we can get this ourselves, by getting some key numbers from our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f5f0b-659f-409e-8fd5-8e81d1281e70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What we do is take the error from our current model, and see how different it is from the previous model. We normalise that difference in error by the difference in the number of predictors between models. For example, if model A has a high error with three predictors, but model B has a very low error but with an additional 25 predictors, we may not be so impressed compared to the same lower error with a model with a single additional predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df378b9-788e-43d4-87bb-cd8f5462ff23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So how do we compare the usefulness of our predictors? We compare the sums of squares residuals of our mean only and sums of squares residuals of our current model, with `kind` as a predictor, to the mean-only model.\n",
    "\n",
    "There are several steps involved:\n",
    "1. Work out the difference between the residual sums of squares in the mean only model, and the current model. We can call this the ss-diff.\n",
    "2. Get the SSR of the current model - this is the error we will compare against.\n",
    "3. Get the residual degrees of freedom in the mean only model, and the current model. The residual degrees of freedom, though fancy-sounding, are represented by the number of observations, minus one, minus the number of predictors in a model. Work out the difference between these two values.\n",
    "4. Normalise the SSR of the current model by the residual degrees of freedom, by division.\n",
    "5. Then divide the ss-diff by the differences in the residual degrees of freedom between the models. These can be thought of as the mean squares difference in error by the mean squares error in the current model.\n",
    "6. Dividing the resulting values gives your $F$ value which can be tested for significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65daa959-ad32-41ec-b72e-9efccbcce5b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That seems like a lot of work. Thankfully, the `anova_lm` function, when called with two arguments, will do all of this for us! Let's see how it performs with the `base_model` and the `with_kind` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b40092fb-7838-4892-b80e-15197afb6ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.0</td>\n",
       "      <td>19648.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>11322.833333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8326.066667</td>\n",
       "      <td>31.987038</td>\n",
       "      <td>3.862193e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid           ssr  df_diff      ss_diff          F        Pr(>F)\n",
       "0      89.0  19648.900000      0.0          NaN        NaN           NaN\n",
       "1      87.0  11322.833333      2.0  8326.066667  31.987038  3.862193e-11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova_lm(base_model, with_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c018c-7387-4e12-a9ca-eb960d425856",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means the difference in $R^2$ - from zero to .42 when including `kind` is a significant result. We can also check this calculation manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb6f3f8-54cb-4062-a721-7cfea481b6e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# What is the difference in residual sums of squares in the models - how big is the difference in error?\n",
    "ss_diff = with_kind.ssr - base_model.ssr\n",
    "\n",
    "# What is the residual SS of the new model?\n",
    "current_ssr = with_kind.ssr\n",
    "\n",
    "# What is the difference in the residual degrees of freedom between models?\n",
    "# ie - how many datapoints are 'left' over in both?\n",
    "model_df_diff = with_kind.df_resid - base_model.df_resid\n",
    "\n",
    "# What is the residual degrees of freedom in our new model?\n",
    "# nobs - number of observations - its also in .df_resid!\n",
    "resid_df = with_kind.nobs - 1 - with_kind.df_model\n",
    "\n",
    "# Normalise our errors\n",
    "mean_squares_error_difference = ss_diff/model_df_diff\n",
    "mean_squares_error = current_ssr/resid_df\n",
    "\n",
    "# Get F\n",
    "F = mean_squares_error_difference/mean_squares_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15f679-9760-444c-b802-328a511de38c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we can check the F value matches the F value computed by our `with_kind` model, as this is what the model was doing to begin with to check the model fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8374077e-9769-4202-b9c5-b11f018c48c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F == with_kind.fvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c09b1e-8f61-43ff-92c0-7079feb6cf2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What ANOVA does\n",
    "When computing a factorial ANOVA, the analysis proceeds by building a stepwise model, examining the change in the amount of variance explained in each step, and computing the significance of it, moving from a model with no predictors to those with more. \n",
    "\n",
    "Let us fit our full model on the `exercise` data, with two predictors `kind` and `diet`, and their interaction, using a linear model, and the compute the ANOVA using `anova_lm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01994c2c-f93c-44a7-b47a-c6bbf5ce844f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8326.067</td>\n",
       "      <td>4163.033</td>\n",
       "      <td>37.824</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>11.465</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind:diet</th>\n",
       "      <td>2.0</td>\n",
       "      <td>815.756</td>\n",
       "      <td>407.878</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>84.0</td>\n",
       "      <td>9245.200</td>\n",
       "      <td>110.062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             df    sum_sq   mean_sq       F  PR(>F)\n",
       "kind        2.0  8326.067  4163.033  37.824   0.000\n",
       "diet        1.0  1261.878  1261.878  11.465   0.001\n",
       "kind:diet   2.0   815.756   407.878   3.706   0.029\n",
       "Residual   84.0  9245.200   110.062     NaN     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit model\n",
    "full_model = smf.ols('pulse ~ 1 + kind + diet + kind:diet', exercise).fit()\n",
    "\n",
    "# Get ANOVA\n",
    "anova = anova_lm(full_model)\n",
    "display(anova.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3d1b8-b5bc-4024-873b-8cb2fc4fb233",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, the question is whether the 'full' model - the one containing both the predictors and their interaction - is a better fit to the data than the models that precede it. The key thing to rememeber is that the model with the interaction has all the predictors in it, while the 'diet' model has kind and diet, with no interaction, and the 'kind' model only has that predictor, and nothing else.\n",
    "\n",
    "First, build all the component models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88703fc6-7e79-4487-9c1e-972da75a5808",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Build the separate step models\n",
    "base_model = smf.ols('pulse ~ 1', exercise).fit()\n",
    "kind = smf.ols('pulse ~ 1 + kind', exercise).fit()\n",
    "kind_diet = smf.ols('pulse ~ 1 + kind + diet', exercise).fit()\n",
    "full = smf.ols('pulse ~ 1 + kind + diet + kind:diet', exercise).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534785a-0c8d-4dfe-a3cd-49caca60c430",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now recreate the analysis of the ANOVA table by comparing models, using the error from the full model (the one with all the predictors and interaction in), and looking at the differences in sums of squares residual between the models, divided by their respective degrees of freedom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eb55e71-b6df-44c8-938d-af74f435c5cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First, get the mean-squared-error from the full model\n",
    "mean_squared_error = full.mse_resid\n",
    "\n",
    "# What is the effect of adding diet to the model, over and above kind?\n",
    "diet_F = ((kind_diet.ssr - kind.ssr) / (kind_diet.df_resid - kind.df_resid)) / mean_squared_error\n",
    "\n",
    "# Or, what about just kind on its own?\n",
    "kind_F = ((kind.ssr - base_model.ssr) / (kind.df_resid - base_model.df_resid)) / mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6ca3d7b-b99b-41ae-b545-5aedf08726ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8326.07</td>\n",
       "      <td>4163.03</td>\n",
       "      <td>37.82</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1261.88</td>\n",
       "      <td>1261.88</td>\n",
       "      <td>11.47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind:diet</th>\n",
       "      <td>2.0</td>\n",
       "      <td>815.76</td>\n",
       "      <td>407.88</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>84.0</td>\n",
       "      <td>9245.20</td>\n",
       "      <td>110.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             df   sum_sq  mean_sq      F  PR(>F)\n",
       "kind        2.0  8326.07  4163.03  37.82    0.00\n",
       "diet        1.0  1261.88  1261.88  11.47    0.00\n",
       "kind:diet   2.0   815.76   407.88   3.71    0.03\n",
       "Residual   84.0  9245.20   110.06    NaN     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.47 37.82\n"
     ]
    }
   ],
   "source": [
    "# Show\n",
    "display(anova.round(2))\n",
    "print(round(diet_F, 2), round(kind_F, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f54498-82a7-46ba-a52b-4e556291f68a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of course, the `anova_lm` function does this is all for you. If you wanted to see the differences in $R^2$ associated with these F tests - the actual effect of interest - we can compute them through simple subtraction of the model R square properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "039c6f0f-3bb6-4d46-9cd8-708f68b05571",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "      <th>R2_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8326.067</td>\n",
       "      <td>4163.033</td>\n",
       "      <td>37.824</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>1261.878</td>\n",
       "      <td>11.465</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind:diet</th>\n",
       "      <td>2.0</td>\n",
       "      <td>815.756</td>\n",
       "      <td>407.878</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>84.0</td>\n",
       "      <td>9245.200</td>\n",
       "      <td>110.062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             df    sum_sq   mean_sq       F  PR(>F)  R2_change\n",
       "kind        2.0  8326.067  4163.033  37.824   0.000      0.424\n",
       "diet        1.0  1261.878  1261.878  11.465   0.001      0.064\n",
       "kind:diet   2.0   815.756   407.878   3.706   0.029      0.042\n",
       "Residual   84.0  9245.200   110.062     NaN     NaN        NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get R squared \n",
    "r_squared = {'kind': kind.rsquared - base_model.rsquared,\n",
    "            'diet': kind_diet.rsquared - kind.rsquared,\n",
    "            'kind:diet': full.rsquared - kind_diet.rsquared\n",
    "            }\n",
    "# Turn this to a series\n",
    "r_sq = pd.Series(r_squared, index=anova.index, name='R2_change')\n",
    "\n",
    "# Add to our anova table\n",
    "full_anova = anova.join(r_sq)\n",
    "display(full_anova.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5be99-6564-4c3a-a5d2-784efd1bf056",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And if we compute the total $R^2$ from this table, it will be equal to the $R^2$ of the full regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48eb3aaa-f775-46f3-9e4a-7fdea3718bc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "r2_sum = full_anova['R2_change'].sum()\n",
    "print(r2_sum == full.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56073e81-a729-4006-8824-72dfc80a3001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Once you have arranged your data into a format that can be easily analysed, then computing a general range of statistical inferences is made easy by the range of packages in Python. The ability to use a single language to complete all analysis steps is a huge plus. The world of statistical inference is enormous and Python is extremely well supported for these kinds of analyses - explore online and the chances are there is a package out there to suit your needs!\n",
    "\n",
    "Good luck in your data adventures with Python!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
